# =============================================================================
# Dynamical Edge Platform v0.7.1 Dependencies
# =============================================================================

# Core ML
torch>=2.7.0                # Required for SAM3 and V-JEPA 2
torchvision>=0.18.0
numpy>=1.24.0

# Hugging Face / Pi0 / VLA Models
transformers>=4.56.0        # Required for DINOv3 HuggingFace integration
accelerate>=0.27.0
protobuf
safetensors
sentencepiece

# =============================================================================
# Pose Estimation (RTMPose)
# =============================================================================
onnxruntime>=1.16.0        # CPU inference
onnxruntime-gpu>=1.16.0    # GPU inference (optional, install separately)
# For model download from HuggingFace
huggingface-hub>=0.20.0

# =============================================================================
# Robot Kinematics & IK (Real Implementation)
# =============================================================================
# Pinocchio - Fast C++ kinematics library (recommended)
# Install with: conda install -c conda-forge pinocchio
# Or: pip install pin (some platforms)

# urchin - Pure Python URDF parser (fallback)
urchin>=0.1.0

# yourdfpy - Alternative URDF loader
yourdfpy>=0.0.56

# =============================================================================
# Depth Estimation (Depth Anything V3)
# =============================================================================
# TensorRT models require: tensorrt (install via NVIDIA SDK)
# ONNX models work with onnxruntime above

# =============================================================================
# Computer Vision
# =============================================================================
opencv-python>=4.8.0
scipy>=1.10.0

# =============================================================================
# Hardware Communication
# =============================================================================
pyserial>=3.5

# =============================================================================
# Web/API Framework
# =============================================================================
fastapi>=0.100.0
uvicorn>=0.23.0
aiohttp>=3.8.0
requests>=2.31.0
httpx>=0.24.0
python-multipart>=0.0.6
websockets>=11.0
jinja2>=3.1.0

# =============================================================================
# Configuration & Utilities
# =============================================================================
psutil>=5.9.0
tqdm>=4.65.0
omegaconf>=2.3.0
hydra-core>=1.3.0
sqlalchemy>=2.0.0
python-dotenv>=1.0.0

# =============================================================================
# Testing
# =============================================================================
pytest>=7.4.0
pytest-asyncio>=0.21.0

# =============================================================================
# Optional: MMPose (for RTMPose model training)
# =============================================================================
# openmim  # Uncomment to install MMPose ecosystem
# mmpose   # pip install -U openmim && mim install mmpose

# =============================================================================
# Optional: TensorRT (for Jetson deployment)
# =============================================================================
# tensorrt  # Install via NVIDIA JetPack or TensorRT package
# pycuda    # Required for TensorRT Python bindings

# =============================================================================
# Optional: Federated Learning / MOAI
# =============================================================================
# tenseal>=0.3.0  # For FHE encryption
# cryptography>=41.0.0

# =============================================================================
# Meta AI Models (DINOv3, SAM3, V-JEPA 2)
# =============================================================================
# These models can be loaded via:
# 1. HuggingFace Transformers (recommended for DINOv3)
# 2. PyTorch Hub (recommended for V-JEPA 2)
# 3. Direct package installation (required for SAM3)

# -----------------------------------------------------------------------------
# DINOv3 - Self-Supervised Vision Foundation Model
# Repository: https://github.com/facebookresearch/dinov3
# -----------------------------------------------------------------------------
# Option A: HuggingFace (easiest - no extra install needed)
#   from transformers import AutoImageProcessor, AutoModel
#   model = AutoModel.from_pretrained("facebook/dinov3-vitl16-pretrain-lvd1689m")
#
# Option B: PyTorch Hub
#   model = torch.hub.load('facebookresearch/dinov3', 'dinov3_vitl16')
#
# Option C: Clone repository for full API
#   git clone https://github.com/facebookresearch/dinov3
#   pip install -e dinov3/

timm>=0.9.0                 # Vision transformer implementations

# -----------------------------------------------------------------------------
# SAM3 - Segment Anything Model 3 (Text-Driven Segmentation)
# Repository: https://github.com/facebookresearch/sam3
# Requires: Python 3.12+, PyTorch 2.7+, CUDA 12.6+
# -----------------------------------------------------------------------------
# Installation:
#   pip install git+https://github.com/facebookresearch/sam3.git
#
# Or clone for development:
#   git clone https://github.com/facebookresearch/sam3
#   cd sam3 && pip install -e .
#
# NOTE: SAM3 checkpoints require HuggingFace authentication:
#   huggingface-cli login
#   # Request access at: https://huggingface.co/facebook/sam3

# -----------------------------------------------------------------------------
# V-JEPA 2 - Video World Model for Robotics
# Repository: https://github.com/facebookresearch/vjepa2
# -----------------------------------------------------------------------------
# Option A: PyTorch Hub (easiest)
#   processor = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_preprocessor')
#   model = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_giant')
#
# Option B: HuggingFace
#   from transformers import AutoVideoProcessor, AutoModel
#   model = AutoModel.from_pretrained("facebook/vjepa2-vitg-fpc64-256")
#
# Option C: Clone for action-conditioned model (robotics)
#   git clone https://github.com/facebookresearch/vjepa2
#   pip install -e vjepa2/
#
# Action-conditioned model for robot planning:
#   encoder, ac_predictor = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_ac_vit_giant')

# NOTE for macOS: V-JEPA 2 requires decord for video. Use eva-decord or decord2:
#   pip install eva-decord

# Shared dependencies for Meta AI models
einops>=0.6.0               # Tensor operations
ftfy>=6.1.0                 # Text processing for CLIP-style models
regex>=2023.0               # Text tokenization

# Optional: Flash Attention for faster inference (Linux only)
# pip install flash-attn --no-build-isolation

# Optional: Decord for video loading (V-JEPA 2)
# pip install decord  # Linux/Windows
# pip install eva-decord  # macOS
